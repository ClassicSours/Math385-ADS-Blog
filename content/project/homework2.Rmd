---
title: homework2
author: Aaron Shaffer
date: '2018-02-17'
slug: homework2
categories: []
tags: ["homework"]
summary: "HW #2: 23.2.1 (#1, #2) , 23.3.3 (#1, #3, #4), 23.4.5 (#1, #3, #4)"
---

<h1>23.2.1 (#1, #2)</h1>

<h4>1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?</h4>

```{r}
set.seed(0)
suppressMessages(library(dplyr))
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)


suppressMessages(library(pander))
sim1a.model <- lm(y ~ x, sim1a)
pander(sim1a.model)

suppressMessages(library(ggplot2))
suppressMessages(library(plotly))

sim1a.df <- as.data.frame(sim1a)
sim1a <- cbind(sim1a, id = 'a')

plot1.gg <- ggplot(aes(x = x, y = y), data = sim1a.df) + geom_point() + geom_line(stat ="smooth", method = "lm")
```

```{r, eval = FALSE}
ggplotly(plot1.gg)
```

<center>
```{r, echo = FALSE}
ggplotly(plot1.gg)
```
</center>

```{r}
simfunction <- function() {
  as.data.frame(
      tibble(
        x = rep(1:10, each = 3),
        y = x * 1.5 + 6 + rt(length(x), df = 2)
    )
  )
}

sims.df <- sim1a
for(letter in letters[-1]) {
  sims.df <- rbind(sims.df, cbind(simfunction(),id = letter))
}

sims.gg <- ggplot(aes(x = x, y = y, group = id, color = id), data = sims.df) +
  geom_point() +
  geom_line(stat ="smooth", method = "lm")
```

```{r, eval = FALSE}
ggplotly(sims.gg)
```

<center>

```{r, echo = FALSE}
ggplotly(sims.gg)
```

</center>



```{r, fig.height = 12, fig.align = 'center', eval = FALSE}
sims.gg +
  facet_wrap(~id, ncol = 4, scales = 'free_y')
```

<center>
```{r, fig.height = 12, fig.align = 'center', echo = FALSE}
sims.gg +
  facet_wrap(~id, ncol = 4, scales = 'free_y')
```
</center>

Expected Y intercept : `6`
Expected slope : `1.5`
```{r}
models <- plyr::dlply(sims.df, "id", function(df) lm(y ~ x, data = df))
pander(models)
```

Some of the models get prety close but when there are one or two values on the high or low end that are extremely off it throws the slope of the linear model completely off.  This is also partially due to the fact that there is only 30 samples so a few extremes throw off the squared values that are used to minimize the linear model.

<h4>2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:</h4>

```{r}
make_prediction <- function(a, data) {
  data$x * a[1] + a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}
```

<h4>Use `optim()' to fit this model to the simulated data above and compare it to the linear model.</h4>

```{r}
pander(optim(c(0,0), measure_distance, data = sim1a))
```

Optim found that an `a[1]` of `1.51` and an `a[2]` of `6.027` on `sim1a` where `lm(y~x)` found an `a[1]` of `1.12` and an `a[2]` of `7.27` on the same input data.

```{r, fig.align='center', fig.height=5, fig.width=12}
abs.model <- plyr::dlply(sims.df, "id", function(df) optim(c(0,0), measure_distance, data = df))
pars <- abs.model[1]$par
for(model in abs.model) {
  pars <- rbind(pars, model$par)
}
pars.lm <- data.frame("lm.m" = double(), "lm.b" = double(), stringsAsFactors = FALSE)
for(model in models) {
  pars.lm <- rbind(pars.lm, model$coefficients)
}

colnames(pars) <- c("optim.m","optim.b")
colnames(pars.lm) <- c("lm.b","lm.m")

pars.all <- cbind(pars, pars.lm, letters)

ggplot(aes(x = as.factor(letters)), data = pars.all) +
  geom_point(aes(y = optim.m, color = 'optim.m')) +
  geom_point(aes(y = lm.m, color = 'lm.m')) +
  geom_point(aes(y = optim.b, color = 'optim.b')) +
  geom_point(aes(y = lm.b, color = 'lm.b')) +
  theme(legend.position = 'top') +
  ylab("Predicted Value") + xlab("Simulation Number")
```
```{r, fig.align='center'}
ggplot(data = pars.all) +
  geom_boxplot(aes(x = 'optim', y = optim.m, color = 'optim.m')) +
  geom_boxplot(aes(x = 'lm', y = lm.m, color = 'lm.m')) +
  geom_boxplot(aes(x = 'optim', y = optim.b, color = 'optim.b')) +
  geom_boxplot(aes(x = 'lm', y = lm.b, color = 'lm.b')) +
  theme(legend.position = 'top') +
  ylab("Predicted Value") + xlab("Prediction Model")
```

`Optim()` with the fitted model vs the linear model also has its own innacuracies.  However as shown from the box plots its calculated values of `m` and `b` for the `Optim()` models much more tigtly wrap the expected values than the `lm()` predictions

23.3.3 (#1, #3, #4)

1. Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using `loess()` instead of `lm()`. How does the result compare to `geom_smooth()`?

```{r, fig.align='center', fig.width=12}
gg.geom_line <- ggplot(aes(x=x,y=y), data = sim1a) +
  geom_line(aes(color = "geom_line, smooth, lm"), stat="smooth", method = "lm", alpha = .5) +
  geom_line(aes(color = "geom_line, smooth, loess"), stat="smooth", method = "loess", alpha = .5) +
  geom_point() +
  theme(legend.position = 'none')

gg.geom_smooth <- ggplot(aes(x=x,y=y), data = sim1a) +
  geom_smooth(method='loess') +
  geom_point()

suppressMessages(library(gridExtra))
grid.arrange(gg.geom_line,gg.geom_smooth, ncol = 2)
```

As far as I can tell the visualization of the line done by `geom_line()` with `method = "loess"` and  `stat = "smooth"` is identical to `geom_smooth()` with the exception being that by default `geom_smooth()` shows a confidence interval around the fitted line, which can be hidden by adding `se = FALSE` to `geom_smooth()`.

```{r}
summary(loess(y~x, sim1a))
```

```{r}
sims.gg.loess <- ggplot(aes(x = x, y = y, group = id, color = id), data = sims.df) +
  geom_point() +
  geom_line(stat ="smooth", method = "loess")
```

```{r, eval = FALSE}
ggplotly(sims.gg.loess)
```

<center>

```{r, echo = FALSE}
ggplotly(sims.gg.loess)
```

</center>

```{r, eval = FALSE}
ggplotly(sims.gg.loess +
  facet_wrap(~id, ncol = 2, scales = 'free_y'))
```

<center>

```{r, echo = FALSE, fig.height=24}
ggplotly(sims.gg.loess +
  facet_wrap(~id, ncol = 2, scales = 'free_y'))
```

</center>

For whatever reason my `y` sim isn't showing up so here it is.

```{r}
y.gg <- sims.df %>%
  filter(id == 'y') %>%
    ggplot(aes(x = x, y = y, color = id)) +
    geom_point() +
    geom_line(stat = "smooth", method = "loess")

ggplotly(y.gg)
```

<h4>3. What does `geom_ref_line()` do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?</h4>

`geom_ref_line()` comes from the `modelr` package.  It is used to add a reference line to a `ggplot2` plot.  It can be useful for putting a line through a value of interest to give a visual reference to how far points are from an expected value.  This is especially useful when the `x` axis is non numeric.


<h4>4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals? </h4>

A frequency poygon of the absolute residuals might give you a better idea of where the true values that your model is predicting might lie.  In theory given a large enough sample size the frequency polygon should be most dense by the true value that your model is looking for.  It can also make it quite visually obvious if there are any glaring issues with your model.  A huge frequency not near zero means that something is wrong in your model or data that you are probably not accounting because the residuals should be especially dense around zero.

```{r}
suppressMessages(library(modelr))
sim1a.res <- sim1a %>% add_residuals(sim1a.model)
ggplot(aes(resid), data = sim1a.res) + geom_freqpoly(binwidth = .5)
```

In this case there is definately a datapoint in sim1a that is way off of what is expected by the simple linear model. But for the most part the `lm()` did a pretty good job with its fit.

23.4.5 (#1, #3, #4)

<h4>1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?</h4>

<h5>With intercept</h5>
```{r, fig.align='center'}
ggplot(sim2) + geom_point(aes(x = x, y = y))

mod <- lm(y ~ x, data = sim2)

pander(mod)

grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod)

pander(grid)

ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)

```


<h5>Without intercept</h5>

```{r}
mod2 <- lm(y ~ 0 + x, data = sim2)

pander(mod2)

grid2 <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)

pander(grid2)

ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid2, aes(y = pred), colour = "red", size = 4)

```

Without the intercept the model of the equation is on `xa` `xb` `xc` and `xd` instead of `intercept` and `x{b,c,d}` however the `t values` are much larger and the `Pr(>|t|)` or levels of signifigance is much much smaller which is better.  

3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod1.func <- function(a,b,c,data) {
  lm(data[[a]] ~ data[[b]] + data[[c]], data = data)
}
mod1.f <- mod1.func(4,1,2,sim3)

pander(mod1)
pander(mod1.f)

mod2 <- lm(y ~ x1 * x2, data = sim3)
mod2.func <- function(a,b,c,data) {
  lm(data[[a]] ~ data[[b]] * data[[c]], data = data)
}
mod2.f <- mod2.func(4,1,2,sim3)

pander(mod2)
pander(mod2.f)
```

<h4>4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but itâ€™s pretty subtle. Can you come up with a plot to support my claim?</h4>

```{r, fig.height = 10,fig.align='center'}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

pander(mod1)
pander(mod2)

grid <- sim4 %>%
  data_grid(
    x1 = seq_range(x1, 10),
    x2 = seq_range(x2, 10)
  ) %>%
  gather_predictions(mod1, mod2)

ggplotly(ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model, ncol = 1))
```

```{r,fig.align='center'}
ggplotly(ggplot(aes(x = x1, y = x2),data = sim4) + geom_point(aes(color = y, size = 2)))
```

Its kind of hard to tell but `mod2` does a better job as its just a tad bit darker in its graph in the upper left quartile which can be seen in the `geom_point` plot above.  This is a little bit hard to tell purely visually but thanks to `plotly::ggplotly()` You can see the higher predictions in that upper quartile in mod2's plot.
